# 神经网络

## 神经网络核心知识总结

该文档围绕神经网络展开，系统涵盖学习目标、网络结构、关键函数（激励函数、损失函数）、梯度下降及网络向量化等核心内容，旨在帮助学习者构建神经网络的基础认知与直观理解。

### 一、学习目标

1. 理解神经网络的基本结构，明确逻辑回归是简化版的网络结构。
2. 掌握神经网络的激励函数、损失函数的概念与作用。
3. 理解逻辑回归中的梯度下降原理，以及神经网络的梯度下降计算方式。
4. 掌握神经网络的网络向量化方法，通过观察学习过程形成对神经网络的直观认知。

### 二、网络结构

#### （一）基础构成

神经网络以神经元为基本单元，文档中展示的简单结构包含输入层（如X₁、X₂、X₃）、计算层与输出层（y），各层通过权重（W）和偏置（b）连接，实现从输入到输出的信息传递与处理。

#### （二）逻辑回归：简化的网络结构

逻辑回归可视为单输出的简化神经网络，其计算流程为：

1. 输入特征（X₁、X₂、X₃）与权重、偏置结合，计算线性结果 ( z = Wx + b )；
2. 通过激励函数（如Sigmoid）对z进行非线性转换，得到激活值 ( a = g(z) )；
3. 激活值a最终作为输出，对应预测结果y。

### 三、激励函数

#### （一）核心作用

为神经网络提供**规模化的非线性化能力**，打破线性模型的局限，使网络能拟合复杂的数据关系。

#### （二）常见类型

1. **Sigmoid函数**：经典激励函数，输出值范围在(0,1)之间，常用于二分类任务的输出层，但存在梯度消失问题。
2. **tanh函数**：输出值范围在(-1,1)之间，相比Sigmoid更接近零均值，能缓解部分梯度问题。
3. **ReLU函数**：计算公式为 ( f(x) = max(0, x) )，即输入大于0时输出自身，小于等于0时输出0；计算简单、不易梯度消失，是目前深度学习中应用最广泛的激励函数之一。

### 四、损失函数

#### （一）作用

用于**评价神经网络预测结果的误差**，量化“预测值”与“真实值”的差距，为后续梯度下降优化提供方向。

#### （二）分类与公式

1. **单次训练损失**：针对单个样本的误差计算，逻辑回归中单次损失公式为：\
   ( L(\hat{y}, y) = -\[y \cdot log(\hat{y}) + (1 - y) \cdot log(1 - \hat{y})] )\
   其中( \hat{y} )为预测值，y为真实标签（取值0或1）。
2. **全部训练损失**：对所有样本的单次损失取平均或求和，反映模型在整个训练集上的整体误差水平。

### 五、梯度下降

#### （一）逻辑回归中的梯度下降

通过计算损失函数对权重（W）和偏置（b）的梯度，迭代更新参数以降低损失，更新公式核心为：

* 偏置更新：( b := b - \alpha \cdot db )（α为学习率，db为损失对b的梯度）；
* 权重更新需结合输入特征与梯度，通过梯度反向传播实现参数优化。

#### （二）神经网络的梯度下降

1. 先通过前向传播计算各层的线性结果( z^n = W^n a^{n-1} + b^n )（n表示层数，( a^{n-1} )为上一层激活值）与激活值( a^n = g(z^n) )；
2. 再通过反向传播计算梯度，关键公式包括：
   * 偏置梯度：( db = dz^n )（( dz^n )为损失对( z^n )的梯度）；
   * 线性结果梯度：( dz^n = da^n \cdot g'(z^n) )（( g'(z^n) )为激励函数的导数）；
   * 权重梯度：( dW^n = dz^n \cdot a^{n-1}^T )（( a^{n-1}^T )为上一层激活值的转置）；
3. 最后根据梯度迭代更新W和b，最小化整体损失。

### 六、网络向量化

#### （一）作用

将神经网络的计算（如前向传播、梯度计算）转化为矩阵运算，替代循环计算，**大幅提升计算效率**，是实现大规模神经网络训练的关键技术。

#### （二）示例流程

以输入层3个特征（X₁、X₂、X₃）、隐藏层4个神经元、输出层1个神经元的网络为例，向量化流程为：

1. 输入向量 ( \begin{bmatrix} x\_1 \ x\_2 \ x\_3 \end{bmatrix} ) 与隐藏层参数（权重矩阵( W\_{3 \times 4}^1 )、偏置向量( b\_{4 \times 1}^1 )）运算，得到隐藏层激活值向量 ( \begin{bmatrix} a\_1^1 \ a\_2^1 \ a\_3^1 \ a\_4^1 \end{bmatrix} )；
2. 隐藏层激活值向量与输出层参数（权重矩阵( W\_{4 \times 1}^2 )、偏置向量( b\_{1 \times 1}^2 )）运算，得到输出层激活值( \[a^2] )，最终对应预测结果y。
